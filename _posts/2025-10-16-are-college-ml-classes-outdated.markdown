---
layout: post
title:  "Are College ML Classes Outdated?"
subtitle: "Some thoughts on a viral tweet that said we're not LLMing enough"
date:   2025-10-16 00:00:00 -0500
categories: jekyll update
---

In the past few days, a tweet by someone who appears to be a Harvard alum has been making rounds on tech/ML Twitter, and it gained enough traction and virality for my dad to send it to me in our family group chat. So, I thought it'd be worth some discussion.

![Harvard and Stanford students tell me their professors don't understand AI and the courses are outdated.  If elite schools can't keep up, the credential arms race is over. Self-learning is the only way now. - Tweet by Harvard Alum Zara Zhang](/assets/article_images/2025-10-16-images/viral_tweet.png "Harvard and Stanford students tell me their professors don't understand AI and the courses are outdated.  If elite schools can't keep up, the credential arms race is over. Self-learning is the only way now. - Tweet by Harvard Alum Zara Zhang")

I think the content of the tweet is *somewhat* true across academic institutions but also reductive in how it's presented. In general, I think students also have a tendency to miss the forest for the trees on a lot of AI/ML pedagogy and fundamentals, and as someone familiar with Stanford's AI/ML courses (they're very similar to Berkeley's), I'm not sure if the problem with them is that they're "outdated."

## What do we mean by "AI?" And should we take students seriously?

At the core of the issues I have with the tweet is actually defining terms. AI is not a new field at all – my dad recalls his alma maters teaching AI courses back when he was in school in the 80s and 90s, and AI/ML staples like <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#:~:text=Image%20recognition%20with%20CNNs%20trained%20by%20gradient%20descent">convolutional neural networks are not a new innovation whatsoever</a> – but in common parlance post-2022, "AI" has become synonymous with "Large Language Models." So when people talk about classrooms not keeping up with AI, they're really talking about AI and ML curricula not sufficiently integrating the last 3 or so years of developments in AI in the curriculum.

Before I respond to this critique, I wanted to begin by saying that the post itself is predicated on us taking this complaint from undergraduates seriously, and I'm of the opinion that in fact, we shouldn't. An undergrad coming out of their degree program feeling unprepared to work with AI in industry is definitely a valid sentiment and hints that *something* in their education was lacking (more on this later), but at the same time, undergraduates generally overindex WAY too much on The Current Thing™️ coming out of college. For starters, LLM workflows are not the *only* thing a computer science or data science student may see in industry that they didn't touch in undergrad, and in fact, they're probably a relatively small fraction of the new-grad software engineering and data science roles in industry. It's funny how we hear this complaint about LLMs specifically, but never hear this complaint about not learning how to use Airflow for workflow orchestration, or not getting to work with Amazon Web Services for cloud infrastructure, or not learning how to handle containerization with Kubernetes and Docker – tools and workflows that are much, *much* more important to the vast majority of technical workflows in industry that don't really have a place in undergraduate curricula. Yet, you never hear people complain about the dearth of education on these tools. It's always about The Current Thing™️.

Now, I myself am admitting that most undergraduate CS/DS programs across the country don't meaningfully teach their students about important non-LLM workflows, so clearly, there must still be something wrong with how we're educating our best and brightest, right? Well, not really! This goes for AI tools and Airflow and cloud infrastructure alike: there's just no feasible way you're going to get to cover every single industry-relevant skill in a *classroom* setting within four years. A lot of that learning will happen on your own independent projects, your internships, and even when you actually start working full-time. My personal view of education is that it's much more important to learn how to think about the "science" of a specific discipline – which could mean understanding time and space complexity for writing efficient algorithms, how to structure databases and manage memory efficiently, or how operating systems and security principles work – than it is to learn a handful of industry-specific tools, many of which might themselves be outdated (or at the very least not widely used) in 10 years!

Also, I think it's important to demystify what "agentic AI workflows" actually are. Most of these systems just come down to writing good prompts (or, let's be real, asking another LLM to write them for you), text parsing for the outputs of these models which you summon with a one-line API call, and good software engineering around this single API call to make some sort of application or system. The models themselves are plug and chug and no one in practice is actually fine tuning models unless you're an AI researcher.

So, in general, I think students tend to overrate what's actually important to take away from their education, and the general student sentiment of "I don’t know how AI works" should be coupled with some self-awareness of what they *want* to take away from the classroom. And that’s a hard question for undergrads to answer because they usually don’t have enough perspective or working experience to know what’s useful or what they want to learn!

But in general, the fact that students feel they don't know enough about AI coming out of college is born more out of anxiety and impostor syndrome than anything materially lacking in their education. It's also only been three years since ChatGPT released and LLM-augmented workflows became more commonplace in industry, so I don't think students should feel particularly worried that in those three years, they spent two of them honing their data structures and algorithms fundamentals.[^1]

## Leveling with the students

But I'll call a spade a spade in one specific regard – it is true that AI (read: LLMs) is a large and growing part of industry[^2], it is true that it is a very exciting and technically rich field that rightfully garners a lot of interest from students, and so naturally, it makes sense for those students to want to work with LLMs in industry. And when you try to recruit for those roles, they will almost always require some *prior history* of working with LLMs, RAG, multi-modal models for things like image generation, AI agents, and the list goes on and on.

These things seem to be missing across most schools' curricula (I think), and so there emerges a mismatch between what students are demanding – more comprehensive "AI courses" that aren't "outdated" as the tweet puts it – and what schools are offering – which are largely still math-intensive classes focused on classical ML algorithms like logistic regression and random forest, and *maybe* a deep learning class that still talks about RNNs and LSTMs in the Big 25 if you're lucky.

Before I talk about the actual coursework offered, I want to also say that I think the LLM-centric view of AI and ML is, in general, a very limiting view of what AI and ML actually are (I'm going to use them interchangeably since the distinction is not too important here). Not every problem in AI is going to be related to LLMs, and there's a lot of value in ML domains besides LLMs; in fact, for many tasks, it's probably cost-efficient to not throw the most expensive and resource-intensive solution right away if simpler models suffice. Your credit card's fraud detector is probably some logistic regression or tree-based model; the software that scans your license plate or passport probably uses a CNN. In most ML engineering settings, the actual problems you work on are not fine-tuning state-of-the-art language models for some deeply masturbatory academic venture; it probably looks a lot more like setting up a DAG on Airflow to put a relatively simple classifier in production.

But I think there is something to be gleaned from student complaints – complaints with which I even resonate – that their college coursework did not really equip them for the new "meta" in industry. My parents seemed to attribute this to academia being under-resourced to keep up with industry, or professors being old and falling back on outdated methods in their pedagogy.

I think this is part of the story but misses the complete picture.

Obviously, one problem is actually that the academics who work on these issues end up going to industry because of opportunity costs; why be a college professor when you could become one of Zuck's coveted supermax AI researchers? But this is still an incomplete view of the pedagogical problem.

To make this concrete, the deep learning class I took at Berkeley sought to tackle the "outdated/not applicable to industry" reputation of the course and hired two instructors from industry – one was an MLE at Pinterest and one was Chief Scientist at InferLink Corporation. But even though you might work in industry, you need the surrounding teaching infrastructure to actually make the class successful, and even those two industry-facing lecturers weren't able to keep up with rapid developments in AI. On the whole, we did not even really touch LLMs in that class and studied deep learning up to transformer developments circa 2021.

That's another part of the issue – the speed at which AI research and industry are moving right now is so fast that any coursework is bound to be on a 3 to 5 year lag to catch up. Hundreds of papers get submitted to conferences every year, and when teaching to classes of hundreds if not thousands of undergraduates, it's extremely difficult to cut through the noise and teach what is "actually important," because what's "actually important" may change in a year. If you don't believe me, when deep learning curriculums finally caught up to teaching RNNs and LSTMs, the innovation of the transformer had already made both of them more-or-less outdated[^3]. But you can imagine that people in 2016 may have been clamoring to add these architectures to ML courses! (For what it's worth, I think LLMs are much more consequential than either RNNs or LSTMs.)

From the pedagogical perspective, why bother teaching something like FAISS or Pinecone or Milvus when they all might be outdated by next year's hot new vector database? I'm not saying *don't* teach these things; rather, I'm trying to argue that it's probably more useful to teach *how and why* something like retrieval works, with focus on ways to measure vector similarity or something like Approximate Nearest Neighbors, methods which are agnostic to the specific industry *tool* that might use them.

Fundamentally for teaching, the problem is standardization. In industry, you have the freedom to try new things, break old things, and keep rapidly iterating until something works, and then retroactively develop some sort of framework that becomes industry-standard. Teaching does not have this privilege, because (1) many of the professors are not used to working in industry settings obviously, and (2) the model of teaching isn't (and shouldn't be) "move fast and break things." There are a lot of time and resources that go into designing good curricula at top schools, and ideally, you want these curricula to be in use for many years (in large part also because professors don't have the bandwidth to keep iterating every semester when big changes happen, and at best you'll get 1 or 2 additional lectures in your ML class on LLMs at the end of the semester because "we kind of have to talk about this").

## Fundamental-focused ML courses ... and things are getting better!

Given these pedagogical constraints and the difficulty of constantly updating to match industry's state-of-the-art, institutions like Berkeley and Stanford opt to focus on "machine learning fundamentals" instead, which, contra the opinions of many students, I actually think is a very good thing. These ideas borrow heavily from "classical" and statistical machine learning, are heavily mathematical and proof-based, and yes, at times feel like they are outdated because most of the focus of the curriculum is not on modern day LLMs.

But these classes are great for two reasons:
1. These tools are "classical," but still see plenty of use in industry ML settings. Like I said, not every problem in AI or ML is going to require LLMs or multimodal generative models, and sometimes proven models like gradient boosting machines for fraud detection or CNNs to read your license plate number for processing are all you need.
2. It's worth knowing how these "classical examples" work because they heavily inform your way of thinking about new models, which fundamentally use the same principles as any other ML model. The people who push progress in AI and ML do not sit around all day playing with the OpenAI API like it's a toy; often the very architectural and modeling decisions they make to innovate come from the mathematical underpinnings that are trained in classical ML classrooms like those at Stanford and Berkeley.

Now, I'm not going to sit here and defend *all* instances of focus on the "classics." At a certain point, content needs to catch up to modern-day methods or it risks becoming stale and generally worthless in any practical setting, research and industry alike. As an example of this, the version of Berkeley's ML class <a href="https://people.eecs.berkeley.edu/~jrs/189s24/">CS 189</a> that I took did not even cover transformers, which does make it meaningfully outdated in a negative way. In the same vein, an <a href="https://www.youtube.com/watch?v=TjZBTDzGeGg&list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi"> MIT AI course that spends more time talking about game trees than LLMs </a> is probably way too stale and should update the curriculum to be significantly more relevant. (Although in fairness, I doubt MIT still uses the same 2010 syllabus that's on YouTube.)

The gap that still needs to be bridged here is that (1) working with LLMs is the new hot thing, (2) being able to work with LLMs requires a professional or industry-facing skillset, and (3) that skillset is not really taught in schools where the focus is much more geared toward "classical" ML. Again, some of this is due to the fundamental lag of fast-paced research, but from my own POV, things are getting better.

But I also think that elite institutions are getting better at addressing this. I took an <a href="https://people.ischool.berkeley.edu/~dbamman/nlp25.html"> NLP class </a> my last semester at Berkeley, and for the first time in the course's iteration, they finally added LLMs to the curriculum, which gave me a lot of really solid foundation on how LLMs work. Berkeley also added an Agentic AI class, and <a href="https://agents4science.github.io/curriculum.html">UChicago has now done the same</a> (I'm enrolled in it)! These classes follow the thesis that, yes, fundamentals are important, but our value proposition is that we're going to present the research papers to you in a distilled fashion and you can go home and implement agentic AI systems so that you actually get practice doing it for industry. Even the PhD ML Theory class I'm now in has largely forsaken spending time on the more classical models, instead choosing 1 or 2 illustrative examples and focusing more on deep learning systems.

For more on Berkeley specifically, see this tweet from Jelani Nelson:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">At <a href="https://twitter.com/Berkeley_EECS?ref_src=twsrc%5Etfw">@Berkeley_EECS</a> we always work to keep our curriculum fresh. Our intro ML course CS 189 just got a drastic makeover this semester (thanks <a href="https://twitter.com/profjoeyg?ref_src=twsrc%5Etfw">@profjoeyg</a> <a href="https://twitter.com/NargesNorouzi?ref_src=twsrc%5Etfw">@NargesNorouzi</a>!) and now includes ~12 lectures on e.g. Adam, PyTorch, various NN architectures, LLMs, and more (see… <a href="https://t.co/XWWvJmT8Dd">https://t.co/XWWvJmT8Dd</a></p>&mdash; Jelani Nelson (@minilek) <a href="https://twitter.com/minilek/status/1978921517024207176?ref_src=twsrc%5Etfw">October 16, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

## So what does this all mean?

The main takeaway here is that there's no need to panic. Yes, a lot of existing ML coursework at even elite schools might feel outdated when they're not talking about LLMs. But: 
1. the focus on the fundamentals is by design – if your goal is to work in AI research, it's probably worth spending some time learning how to take gradients to solve optimization problems or learn how clustering and nearest neighbor methods work (these are also used in LLM systems!)
2. what people mean by "outdated" could either mean "theoretical and mathematical" (not necessarily bad) or "not enough hands-on practice with industry-relevant skills" (definitely bad), and the onus is both on students to find courses that satisfy either their academic or pre-professional needs and on institutions to provide coursework for both these goals
3. curricula *are* getting updated at schools like <a href="https://www.youtube.com/watch?v=2fq9wYslV0A&list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16">Stanford</a> and Berkeley alike (I don't know enough about OP's alma mater Harvard unfortunately). It's not exactly true that LLMs are new – GPT-1 released back in 2018 – but it is true they've garnered a lot more attention since the release of ChatGPT in late-2022, and given their relatively recent rise, I expect that within the next few years we'll have the first set of standardized LLM pedagogy across elite schools.

## A final word about "careerism"

One other small gripe I have with the post is that it does a little bit of question begging that the purpose of schools and education is to equip students with these pre-professional, industry-facing skills for AI. (And it also assumes that <a href="https://x.com/zarazhangrui/status/1978488825052840188">self-learning by watching YouTube</a> is somehow a universal remedy for this, which ... lol, look at how <a href="https://www.youtube.com/watch?v=0JUN9aDxVmI&list=PL2SOU6wwxB0uP4rJgf5ayhHWgw7akUWSf">the views on Prof. Nelson's algos playlist from Harvard decline with each video.</a>)

This may fall on deaf ears because plenty of people disagree, but in my own personal philosophy of what education should be, I hate the focus on careerism, that the purpose of learning is to equip you with whatever set of tools are most hire-able at the current point in time. This is just profoundly stupid to me and comes from failing to understand the difference between "skills" (like coding, or speaking Spanish) and "science" (like computer science, or the "art" of Spanish, so to speak). For more on this, go read ryxcommar's blogpost <a href="https://ryxcommar.com/2020/05/15/coding-is-not-computer-science/">"Coding is Not Computer Science."</a>

I don't want to go too deeply on this in this article (perhaps it warrants its own article), but the idea that someone's computer science education should just consist of them learning various programming languages and doing agentic AI projects for 4 years seems like a supreme disservice to their education. Sure, they'll probably be great junior level coders in industry for one very specific type of role that gets a lot of media hype! Keep calling those APIs, champ! But what happens when a senior engineer comes to them asking why their RAG system can't scale beyond a few users? Or when they need to design a database schema from scratch and don't understand normalization and indexing? Or when they have to architect a system that's reliable and secure, not just functional, and they don't have a mental model of how authentication and access control actually work?

I'm not saying that coursework should *only* be theory focused – at the end of the day, you do want to graduate people who are employable and know how to *do* things (like code) – but there should be a balance, and you almost never hear the careerist folks who dismiss ML education as "outdated" because intro classes teach mathematical proofs stop to think about what the merits of doing so might be. Balance is important, but for some reason, it's the people who know that the purpose of education should be on *how to think* and not *what to do* who are also on the backfoot defending pedagogical approaches that aren't just equipping mindless junior SWEs with a skillset that amounts to just calling a handful of APIs. (If you don't believe me, ask how many data scientists are using *pandas* routinely on the job.)

So, forgive me for thinking that AI and ML education should be a little bit more holistic than "spend half the time talking about LLMs." I promise you that learning about logistic regression or K-nearest neighbors are not worthless, outdated exercises, and it would probably benefit the vast majority of people who want to work with LLMs to actually engage with the fundamental principles of machine learning on a deeper level before complaining about how their ML education was lacking. It's not like you would go to lecture either way, buddy :)

---
#### Footnotes
[^1]: <small>I will add that it certainly doesn't help that every dimwit on LinkedIn posts about the rise of agentic AI !! and RAG !! and how in <a href="https://www.linkedin.com/posts/dainis-tka_in-3-to-6-months-ai-will-write-90-of-the-activity-7305555036363722752-QDTg/"> 3 to 6 months AI will be writing 90% of code </a> so either learn it or get left behind. Certainly doesn't assuage anyone's fears when the hype cycle is this loud.</small>
[^2]: <small>And may also be <a href="https://www.slowboring.com/p/the-ai-boom-is-propping-up-the-whole">single-handedly propping up economic growth!</a></small>
[^3]: <small>Some people might take issue with me claiming this, but I like to fall back on <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">Richard Sutton's Bitter Lesson</a>, which is that incremental architectural improvements, while certainly clever, are not what drives innovation in machine learning. The bitter lesson of the lsat 7 decades of AI research is that general methods and flexible models that leverage boat loads of data with strong computation ultimately succeed most. There's really only three big players in modern ML, in my opinion – the vanilla deep neural network, CNNs, and the various flavors of transformers.</small>